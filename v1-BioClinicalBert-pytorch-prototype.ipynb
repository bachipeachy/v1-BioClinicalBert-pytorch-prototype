{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bio_ClinicalBERT_prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time, math\n",
    "import numpy as np\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys/python version -> 3.7.7 (default, Mar 10 2020, 15:43:33) \n",
      "[Clang 11.0.0 (clang-1100.0.33.17)]\n"
     ]
    }
   ],
   "source": [
    "print(\"sys/python version ->\", sys.version)\n",
    "np.set_printoptions(suppress=True, precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "svrcm_file = './data/clinical_notes.csv'\n",
    "bert_path = \"./biobert_pretrain_output_all_notes_150000\"\n",
    "\n",
    "SEQUENCE_LEN = 200\n",
    "TRAIN_BATCH_SIZE = 6\n",
    "EPOCHS = 1\n",
    "\n",
    "CATEGORIES = ['allergies',\n",
    " 'chief_complaint',\n",
    " 'cpt_code',\n",
    " 'current_medication',\n",
    " 'diag_code',\n",
    " 'examination',\n",
    " 'fam_hist',\n",
    " 'hosp_hist',\n",
    " 'illness_hist',\n",
    " 'med_hist',\n",
    " 'modifier',\n",
    " 'social_hist',\n",
    " 'surg_hist',\n",
    " 'uncategorized']\n",
    "\n",
    "MODEL_NAMES = ['burt_only', 'burt_ltsm']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_parameters(model):\n",
    "    params = list(model.named_parameters())\n",
    "    trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print('model has {:} different named parameters.'.format(len(params)))\n",
    "    print(\"trainable parameters:\", trainable_parameters, '\\n')\n",
    "\n",
    "    \n",
    "    print('==== Embedding Layer ====\\n')\n",
    "    for p in params[0:5]:\n",
    "        print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "    print('\\n==== First Transformer (of possible twelve) ====\\n')\n",
    "    for p in params[5:21]:\n",
    "        print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "    print('\\n==== Output Layer ====\\n')\n",
    "    for p in params[-4:]:\n",
    "        print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioClinicalBert(nn.Module):\n",
    "    \n",
    "    def __init__(self, bert_path):\n",
    "        super(BioClinicalBert, self).__init__()\n",
    "        self.bert_path = bert_path\n",
    "        self.bert_model = transformers.BertModel.from_pretrained(bert_path)\n",
    "        self.bert_drop = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(768, len(CATEGORIES))\n",
    "        self.activation = nn.Softmax(dim=1)\n",
    "\n",
    "    \n",
    "    def forward(self, ids, masks, token_type_ids):\n",
    "        _, pooled = self.bert_model(ids, masks, token_type_ids)\n",
    "        do_output = self.bert_drop(pooled)\n",
    "        fc_output = self.fc(do_output)\n",
    "        output = self.activation(fc_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioClinicalBertLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, bert_path):\n",
    "        super(BioClinicalBertLSTM, self).__init__()\n",
    "        self.bert_path = bert_path\n",
    "        \n",
    "        self.bert_model = transformers.BertModel.from_pretrained(bert_path)\n",
    "        self.bert_drop = nn.Dropout(0.1)\n",
    "        self.lstm = nn.LSTM(768, 128, \n",
    "                       num_layers=1, \n",
    "                       bidirectional=True, \n",
    "                       batch_first=True)\n",
    "        self.fc = nn.Linear(2*128, len(CATEGORIES))\n",
    "        self.activation = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, ids, masks, token_type_ids):\n",
    "        unpooled, pooled = self.bert_model(ids, masks, token_type_ids)\n",
    "        do_output = self.bert_drop(unpooled)\n",
    "        lstm_output, (hidden, cell) = self.lstm(do_output)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        fc_output = self.fc(hidden)\n",
    "        output = self.activation(fc_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_burt_only = BioClinicalBert(bert_path)\n",
    "model_burt_ltsm = BioClinicalBertLSTM(bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has 201 different named parameters.\n",
      "trainable parameters: 108321038 \n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert_model.embeddings.word_embeddings.weight            (28996, 768)\n",
      "bert_model.embeddings.position_embeddings.weight          (512, 768)\n",
      "bert_model.embeddings.token_type_embeddings.weight          (2, 768)\n",
      "bert_model.embeddings.LayerNorm.weight                        (768,)\n",
      "bert_model.embeddings.LayerNorm.bias                          (768,)\n",
      "\n",
      "==== First Transformer (of possible twelve) ====\n",
      "\n",
      "bert_model.encoder.layer.0.attention.self.query.weight    (768, 768)\n",
      "bert_model.encoder.layer.0.attention.self.query.bias          (768,)\n",
      "bert_model.encoder.layer.0.attention.self.key.weight      (768, 768)\n",
      "bert_model.encoder.layer.0.attention.self.key.bias            (768,)\n",
      "bert_model.encoder.layer.0.attention.self.value.weight    (768, 768)\n",
      "bert_model.encoder.layer.0.attention.self.value.bias          (768,)\n",
      "bert_model.encoder.layer.0.attention.output.dense.weight   (768, 768)\n",
      "bert_model.encoder.layer.0.attention.output.dense.bias        (768,)\n",
      "bert_model.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n",
      "bert_model.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n",
      "bert_model.encoder.layer.0.intermediate.dense.weight     (3072, 768)\n",
      "bert_model.encoder.layer.0.intermediate.dense.bias           (3072,)\n",
      "bert_model.encoder.layer.0.output.dense.weight           (768, 3072)\n",
      "bert_model.encoder.layer.0.output.dense.bias                  (768,)\n",
      "bert_model.encoder.layer.0.output.LayerNorm.weight            (768,)\n",
      "bert_model.encoder.layer.0.output.LayerNorm.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert_model.pooler.dense.weight                            (768, 768)\n",
      "bert_model.pooler.dense.bias                                  (768,)\n",
      "fc.weight                                                  (14, 768)\n",
      "fc.bias                                                        (14,)\n"
     ]
    }
   ],
   "source": [
    "get_model_parameters(model_burt_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has 209 different named parameters.\n",
      "trainable parameters: 109233422 \n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert_model.embeddings.word_embeddings.weight            (28996, 768)\n",
      "bert_model.embeddings.position_embeddings.weight          (512, 768)\n",
      "bert_model.embeddings.token_type_embeddings.weight          (2, 768)\n",
      "bert_model.embeddings.LayerNorm.weight                        (768,)\n",
      "bert_model.embeddings.LayerNorm.bias                          (768,)\n",
      "\n",
      "==== First Transformer (of possible twelve) ====\n",
      "\n",
      "bert_model.encoder.layer.0.attention.self.query.weight    (768, 768)\n",
      "bert_model.encoder.layer.0.attention.self.query.bias          (768,)\n",
      "bert_model.encoder.layer.0.attention.self.key.weight      (768, 768)\n",
      "bert_model.encoder.layer.0.attention.self.key.bias            (768,)\n",
      "bert_model.encoder.layer.0.attention.self.value.weight    (768, 768)\n",
      "bert_model.encoder.layer.0.attention.self.value.bias          (768,)\n",
      "bert_model.encoder.layer.0.attention.output.dense.weight   (768, 768)\n",
      "bert_model.encoder.layer.0.attention.output.dense.bias        (768,)\n",
      "bert_model.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n",
      "bert_model.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n",
      "bert_model.encoder.layer.0.intermediate.dense.weight     (3072, 768)\n",
      "bert_model.encoder.layer.0.intermediate.dense.bias           (3072,)\n",
      "bert_model.encoder.layer.0.output.dense.weight           (768, 3072)\n",
      "bert_model.encoder.layer.0.output.dense.bias                  (768,)\n",
      "bert_model.encoder.layer.0.output.LayerNorm.weight            (768,)\n",
      "bert_model.encoder.layer.0.output.LayerNorm.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "lstm.bias_ih_l0_reverse                                       (512,)\n",
      "lstm.bias_hh_l0_reverse                                       (512,)\n",
      "fc.weight                                                  (14, 256)\n",
      "fc.bias                                                        (14,)\n"
     ]
    }
   ],
   "source": [
    "get_model_parameters(model_burt_ltsm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size -> 28996\n",
      "testing access to vocabulary\n",
      "  tokenizer.convert_tokens_to_ids['vision'] -> [4152]\n",
      "  tokenizer.convert_ids_to_tokens(4152] -> ['vision']\n"
     ]
    }
   ],
   "source": [
    "def get_tokenizer():\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained(bert_path)\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    print(\"vocab_size ->\", vocab_size)\n",
    "    print(\"testing access to vocabulary\")\n",
    "    print(\"  tokenizer.convert_tokens_to_ids['vision'] ->\", tokenizer.convert_tokens_to_ids(['vision']))\n",
    "    print(\"  tokenizer.convert_ids_to_tokens(4152] ->\" , tokenizer.convert_ids_to_tokens([4152]))\n",
    "    return tokenizer, vocab_size\n",
    "\n",
    "tokenizer, vocab_size = get_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clinical_notes Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "svrcm_file = './data/clinical_notes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LEN = 200\n",
    "assert SEQUENCE_LEN < 512, \"The maximun permissible sequence length for BERT is 512\"\n",
    "\n",
    "CATEGORIES = ['allergies',\n",
    " 'chief_complaint',\n",
    " 'cpt_code',\n",
    " 'current_medication',\n",
    " 'diag_code',\n",
    " 'examination',\n",
    " 'fam_hist',\n",
    " 'hosp_hist',\n",
    " 'illness_hist',\n",
    " 'med_hist',\n",
    " 'modifier',\n",
    " 'social_hist',\n",
    " 'surg_hist',\n",
    " 'uncategorized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_categories(df):\n",
    "    values = df.values\n",
    "    values = [line.strip() for line in values]\n",
    "    arr_binary = np.in1d(values, CATEGORIES)\n",
    "    bad_rows = np.flatnonzero(arr_binary == False)\n",
    "    excel_bad_rows = bad_rows+2\n",
    "    if excel_bad_rows.size>0 :\n",
    "        print(\"DATA ERRORS -> clinical_notes.csv file as following excel rows with bad categories:\\n\",\n",
    "              excel_bad_rows)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_clinical_notes(df):\n",
    "    len_arr = [len(note) for note in df]\n",
    "    len_arr.sort()\n",
    "    print(\"df_notes -> shape {}, min_len {}, max_len {}, mean {}, median {}\".format(\n",
    "        df.shape, len_arr[0], len_arr[-1],\n",
    "        int(statistics.mean(len_arr)), int(statistics.median(len_arr))))\n",
    "    assert  len_arr[-1] < SEQUENCE_LEN, \"notes include a line greater than max allowed\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_notes -> shape (210,), min_len 2, max_len 175, mean 59, median 54\n"
     ]
    }
   ],
   "source": [
    "def download_clinical_notes(svrcm_file):\n",
    "    df_notes = pd.read_csv(svrcm_file, engine='python')\n",
    "    validate_categories(df_notes['category'])\n",
    "    validate_clinical_notes(df_notes['notes'])\n",
    "    \n",
    "    return df_notes\n",
    "\n",
    "df_notes = download_clinical_notes(svrcm_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_batch(patient_id, sentence):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        sentence, add_special_tokens = True, max_length = SEQUENCE_LEN, pad_to_max_length = True,\n",
    "        return_attention_mask = True, return_tensors = 'pt',)        \n",
    "    return encoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids -> <class 'list'>, 210\n",
      "input_ids[0] -> <class 'dict'>, 3\n"
     ]
    }
   ],
   "source": [
    "def vectorize_dataset(df):\n",
    "    input_ids = []\n",
    "    \n",
    "    for i, notes in enumerate(df['notes']):\n",
    "        patient_id = df.id[i]\n",
    "        encoded_dict = vectorize_batch(patient_id, notes)\n",
    "        input_ids.append(encoded_dict)\n",
    "   \n",
    "    print(\"input_ids -> {}, {}\\ninput_ids[0] -> {}, {}\".format(\n",
    "        type(input_ids), len(input_ids), type(input_ids[0]), len(input_ids[0])))\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "input_ids = vectorize_dataset(df_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      chief_complaint\n",
       "1      chief_complaint\n",
       "2      chief_complaint\n",
       "3         illness_hist\n",
       "4      chief_complaint\n",
       "            ...       \n",
       "205        examination\n",
       "206        examination\n",
       "207        examination\n",
       "208          diag_code\n",
       "209           modifier\n",
       "Name: category, Length: 210, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notes.category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['allergies',\n",
       " 'chief_complaint',\n",
       " 'cpt_code',\n",
       " 'current_medication',\n",
       " 'diag_code',\n",
       " 'examination',\n",
       " 'fam_hist',\n",
       " 'hosp_hist',\n",
       " 'illness_hist',\n",
       " 'med_hist',\n",
       " 'modifier',\n",
       " 'social_hist',\n",
       " 'surg_hist',\n",
       " 'uncategorized']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CATEGORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_encode_category(df):\n",
    "    labels = []\n",
    "    for cat in df.category:\n",
    "        labels.append(CATEGORIES.index(cat))\n",
    "    print(\"labels -> {} {}\".format(type(labels), len(labels)))\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels -> <class 'list'> 210\n"
     ]
    }
   ],
   "source": [
    "labels = idx_encode_category(df_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids          :  <class 'list'> 210\n",
      "input_ids[7]       :  <class 'dict'> 3\n",
      "labels             :  <class 'list'> 210\n"
     ]
    }
   ],
   "source": [
    "print(\"input_ids          : \", type(input_ids), len(input_ids))\n",
    "print(\"input_ids[7]       : \", type(input_ids[7]), len(input_ids[7]))\n",
    "print(\"labels             : \", type(labels), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_dataset (patients, col)-> (9, 5)\n"
     ]
    }
   ],
   "source": [
    "def create_patient_dataset(df):\n",
    "    df['input_ids'] = input_ids\n",
    "    df['labels'] = labels\n",
    "\n",
    "    df_1 = df.groupby(['id'])['notes'].apply(list)\n",
    "    df_2 = df.groupby(['id'])['category'].apply(list)\n",
    "    df_3 = df.groupby(['id'])['input_ids'].apply(list)\n",
    "    df_4 = df.groupby(['id'])['labels'].apply(list)\n",
    "    \n",
    "    df_dataset = pd.concat([df_1, df_2], axis=1)\n",
    "    df_dataset = pd.concat([df_dataset, df_3], axis=1)\n",
    "    df_dataset = pd.concat([df_dataset, df_4], axis=1)\n",
    "    df_dataset.reset_index(inplace=True)\n",
    "\n",
    "    assert (df_dataset.shape[0]==len(df.id.unique()) and\n",
    "            df_dataset.shape[1]==5 ), \"df_dataset shape mismatch\"\n",
    "    print(\"df_dataset (patients, col)->\", df_dataset.shape)\n",
    "    return df_dataset\n",
    "\n",
    "df_dataset = create_patient_dataset(df_notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device to run on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_burt_only = model_burt_only.to(device)\n",
    "model_burt_ltsm = model_burt_ltsm.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions with no training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model):\n",
    "    preds = []\n",
    "    preds_max_idx = []\n",
    "    preds_cat = []\n",
    "    \n",
    "    for i, sample in enumerate(input_ids):\n",
    "        pred = model(\n",
    "            input_ids[i]['input_ids'],\n",
    "            input_ids[i]['token_type_ids'],\n",
    "            input_ids[i]['attention_mask'])\n",
    "        \n",
    "        pred = pred.cpu().detach().numpy().flatten()\n",
    "        pred_idx = np.argmax(pred)\n",
    "        pred_cat = CATEGORIES[pred_idx]\n",
    "        \n",
    "        softmax_sum = np.around(np.sum(pred), decimals=0)\n",
    "        assert  softmax_sum == 1, \"softmax does not addup to 1\"\n",
    "        preds.append(pred)\n",
    "        preds_max_idx.append(pred_idx)\n",
    "        preds_cat.append(pred_cat)\n",
    "\n",
    "    return preds, preds_max_idx, preds_cat\n",
    "\n",
    "                                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_0, preds_max_idx_0, preds_cat_0 = predict(model_burt_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_1, preds_max_idx_1, preds_cat_1 = predict(model_burt_ltsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "burt_only_pct success 7% with following rows# that passed\n",
      "     4 : 100\n",
      "     9 : 100\n",
      "     43 : 100\n",
      "     44 : 100\n",
      "     51 : 100\n",
      "     78 : 100\n",
      "     116 : 100\n",
      "     119 : 100\n",
      "     140 : 100\n",
      "     141 : 100\n",
      "     148 : 100\n",
      "     161 : 100\n",
      "     162 : 100\n",
      "     193 : 100\n",
      "\n",
      "burt_ltsm_pct success 1% with following row# that passed\n",
      "\n",
      "     51 : 100\n",
      "     120 : 100\n",
      "     155 : 100\n"
     ]
    }
   ],
   "source": [
    "def result():\n",
    "    dict_result = {\"id\" : df_notes.id,\n",
    "            \"true_cat\": df_notes.category,\n",
    "            \"burt_only_pred\" : preds_cat_0,\n",
    "            \"burt_only_score\": None,\n",
    "            \"burt_ltsm_pred\" : preds_cat_1,\n",
    "            \"burt_ltsm_score\" : None}\n",
    "    \n",
    "    df_result = pd.DataFrame(dict_result)\n",
    "\n",
    "    df_result.burt_only_score = np.where(\n",
    "        df_result.true_cat == df_result.burt_only_pred, 100, 0)\n",
    "    df_result.burt_ltsm_score = np.where(\n",
    "        df_result.true_cat == df_result.burt_ltsm_pred, 100, 0)\n",
    "    \n",
    "    burt_only_cumsum = (df_result.burt_only_score==100).sum()\n",
    "    burt_only_pct = burt_only_cumsum/len(df_result.burt_only_score)\n",
    "    \n",
    "    burt_ltsm_cumsum = (df_result.burt_ltsm_score==100).sum()\n",
    "    burt_ltsm_pct = burt_ltsm_cumsum/len(df_result.burt_ltsm_score)\n",
    "\n",
    "    \n",
    "    print(\"burt_only_pct success {:.0%} with following rows# that passed\".format(burt_only_pct))\n",
    "    [print(\"    \", i, \":\", row) for i, row in enumerate(\n",
    "        df_result.burt_only_score) if row==100]\n",
    "\n",
    "    print(\"\\nburt_ltsm_pct success {:.0%} with following row# that passed\\n\".format(burt_ltsm_pct))\n",
    "    [print(\"    \", i, \":\", row) for i, row in enumerate(\n",
    "        df_result.burt_ltsm_score) if row==100]\n",
    "\n",
    "    return df_result\n",
    "\n",
    "df_result = result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>true_cat</th>\n",
       "      <th>burt_only_pred</th>\n",
       "      <th>burt_only_score</th>\n",
       "      <th>burt_ltsm_pred</th>\n",
       "      <th>burt_ltsm_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11111</td>\n",
       "      <td>chief_complaint</td>\n",
       "      <td>social_hist</td>\n",
       "      <td>0</td>\n",
       "      <td>social_hist</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11111</td>\n",
       "      <td>chief_complaint</td>\n",
       "      <td>modifier</td>\n",
       "      <td>0</td>\n",
       "      <td>social_hist</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11111</td>\n",
       "      <td>chief_complaint</td>\n",
       "      <td>uncategorized</td>\n",
       "      <td>0</td>\n",
       "      <td>uncategorized</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11111</td>\n",
       "      <td>illness_hist</td>\n",
       "      <td>diag_code</td>\n",
       "      <td>0</td>\n",
       "      <td>social_hist</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11111</td>\n",
       "      <td>chief_complaint</td>\n",
       "      <td>chief_complaint</td>\n",
       "      <td>100</td>\n",
       "      <td>uncategorized</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>81118</td>\n",
       "      <td>examination</td>\n",
       "      <td>social_hist</td>\n",
       "      <td>0</td>\n",
       "      <td>uncategorized</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>81118</td>\n",
       "      <td>examination</td>\n",
       "      <td>uncategorized</td>\n",
       "      <td>0</td>\n",
       "      <td>uncategorized</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>81118</td>\n",
       "      <td>examination</td>\n",
       "      <td>uncategorized</td>\n",
       "      <td>0</td>\n",
       "      <td>hosp_hist</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>81118</td>\n",
       "      <td>diag_code</td>\n",
       "      <td>med_hist</td>\n",
       "      <td>0</td>\n",
       "      <td>uncategorized</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>81118</td>\n",
       "      <td>modifier</td>\n",
       "      <td>allergies</td>\n",
       "      <td>0</td>\n",
       "      <td>uncategorized</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id         true_cat   burt_only_pred  burt_only_score burt_ltsm_pred  \\\n",
       "0    11111  chief_complaint      social_hist                0    social_hist   \n",
       "1    11111  chief_complaint         modifier                0    social_hist   \n",
       "2    11111  chief_complaint    uncategorized                0  uncategorized   \n",
       "3    11111     illness_hist        diag_code                0    social_hist   \n",
       "4    11111  chief_complaint  chief_complaint              100  uncategorized   \n",
       "..     ...              ...              ...              ...            ...   \n",
       "205  81118      examination      social_hist                0  uncategorized   \n",
       "206  81118      examination    uncategorized                0  uncategorized   \n",
       "207  81118      examination    uncategorized                0      hosp_hist   \n",
       "208  81118        diag_code         med_hist                0  uncategorized   \n",
       "209  81118         modifier        allergies                0  uncategorized   \n",
       "\n",
       "     burt_ltsm_score  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  \n",
       "..               ...  \n",
       "205                0  \n",
       "206                0  \n",
       "207                0  \n",
       "208                0  \n",
       "209                0  \n",
       "\n",
       "[210 rows x 6 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'burt_only': AdamW (\n",
       " Parameter Group 0\n",
       "     betas: (0.9, 0.999)\n",
       "     correct_bias: True\n",
       "     eps: 1e-06\n",
       "     lr: 2e-05\n",
       "     weight_decay: 0.0\n",
       " ),\n",
       " 'burt_ltsm': AdamW (\n",
       " Parameter Group 0\n",
       "     betas: (0.9, 0.999)\n",
       "     correct_bias: True\n",
       "     eps: 1e-06\n",
       "     lr: 0.005\n",
       "     weight_decay: 0.0\n",
       " )}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim_dict = { MODEL_NAMES[0] : AdamW(model_burt_only.parameters(), lr=2e-5),\n",
    "                   MODEL_NAMES[1] : AdamW(model_burt_ltsm.parameters(), lr=5e-3)}\n",
    "\n",
    "optim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs==labels)/float(labels.size)\n",
    "\n",
    "metrics_dict = { 'accuracy': accuracy, 'loss' : 'loss'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': <function __main__.accuracy(out, labels)>, 'loss': 'loss'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(dic):\n",
    "    pred = model_burt_only(\n",
    "        dic['input_ids'],\n",
    "        dic['token_type_ids'],\n",
    "        dic['attention_mask'])\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu\n",
      "starting epoch# 1 ...\n",
      "0 -> num sampled [52] avg_loss per sample: 0.051,   pct succeeded: 1.9%\n",
      "1 -> num sampled [29] avg_loss per sample: 0.091,   pct succeeded: 0.0%\n",
      "2 -> num sampled [36] avg_loss per sample: 0.074,   pct succeeded: 0.0%\n",
      "3 -> num sampled [34] avg_loss per sample: 0.078,   pct succeeded: 5.9%\n",
      "4 -> num sampled [12] avg_loss per sample: 0.221,   pct succeeded: 0.0%\n",
      "5 -> num sampled [17] avg_loss per sample: 0.156,   pct succeeded: 0.0%\n",
      "6 -> num sampled [14] avg_loss per sample: 0.190,   pct succeeded: 0.0%\n",
      "7 -> num sampled [5] avg_loss per sample: 0.528,   pct succeeded: 0.0%\n",
      "8 -> num sampled [11] avg_loss per sample: 0.241,   pct succeeded: 0.0%\n",
      "epoch 1, avg loss per sample 0.1136, train acc 1.4%, time 50.6 sec\n",
      "\n",
      "starting epoch# 2 ...\n",
      "0 -> num sampled [52] avg_loss per sample: 0.051,   pct succeeded: 3.8%\n",
      "1 -> num sampled [29] avg_loss per sample: 0.091,   pct succeeded: 6.9%\n",
      "2 -> num sampled [36] avg_loss per sample: 0.074,   pct succeeded: 2.8%\n",
      "3 -> num sampled [34] avg_loss per sample: 0.078,   pct succeeded: 5.9%\n",
      "4 -> num sampled [12] avg_loss per sample: 0.221,   pct succeeded: 8.3%\n",
      "5 -> num sampled [17] avg_loss per sample: 0.156,   pct succeeded: 5.9%\n",
      "6 -> num sampled [14] avg_loss per sample: 0.190,   pct succeeded: 0.0%\n",
      "7 -> num sampled [5] avg_loss per sample: 0.526,   pct succeeded: 0.0%\n",
      "8 -> num sampled [11] avg_loss per sample: 0.241,   pct succeeded: 18.2%\n",
      "epoch 2, avg loss per sample 0.1135, train acc 5.2%, time 52.6 sec\n",
      "\n",
      "starting epoch# 3 ...\n",
      "0 -> num sampled [52] avg_loss per sample: 0.051,   pct succeeded: 5.8%\n",
      "1 -> num sampled [29] avg_loss per sample: 0.091,   pct succeeded: 0.0%\n",
      "2 -> num sampled [36] avg_loss per sample: 0.074,   pct succeeded: 0.0%\n",
      "3 -> num sampled [34] avg_loss per sample: 0.078,   pct succeeded: 2.9%\n",
      "4 -> num sampled [12] avg_loss per sample: 0.221,   pct succeeded: 0.0%\n",
      "5 -> num sampled [17] avg_loss per sample: 0.156,   pct succeeded: 5.9%\n",
      "6 -> num sampled [14] avg_loss per sample: 0.189,   pct succeeded: 0.0%\n",
      "7 -> num sampled [5] avg_loss per sample: 0.527,   pct succeeded: 0.0%\n",
      "8 -> num sampled [11] avg_loss per sample: 0.241,   pct succeeded: 9.1%\n",
      "epoch 3, avg loss per sample 0.1135, train acc 2.9%, time 52.5 sec\n",
      "\n",
      "starting epoch# 4 ...\n",
      "0 -> num sampled [52] avg_loss per sample: 0.051,   pct succeeded: 7.7%\n",
      "1 -> num sampled [29] avg_loss per sample: 0.091,   pct succeeded: 3.4%\n",
      "2 -> num sampled [36] avg_loss per sample: 0.074,   pct succeeded: 2.8%\n",
      "3 -> num sampled [34] avg_loss per sample: 0.078,   pct succeeded: 2.9%\n",
      "4 -> num sampled [12] avg_loss per sample: 0.220,   pct succeeded: 8.3%\n",
      "5 -> num sampled [17] avg_loss per sample: 0.156,   pct succeeded: 5.9%\n",
      "6 -> num sampled [14] avg_loss per sample: 0.189,   pct succeeded: 0.0%\n",
      "7 -> num sampled [5] avg_loss per sample: 0.531,   pct succeeded: 0.0%\n",
      "8 -> num sampled [11] avg_loss per sample: 0.240,   pct succeeded: 27.3%\n",
      "epoch 4, avg loss per sample 0.1135, train acc 5.7%, time 50.1 sec\n",
      "\n",
      "starting epoch# 5 ...\n",
      "0 -> num sampled [52] avg_loss per sample: 0.051,   pct succeeded: 7.7%\n",
      "1 -> num sampled [29] avg_loss per sample: 0.091,   pct succeeded: 0.0%\n",
      "2 -> num sampled [36] avg_loss per sample: 0.074,   pct succeeded: 0.0%\n",
      "3 -> num sampled [34] avg_loss per sample: 0.078,   pct succeeded: 5.9%\n",
      "4 -> num sampled [12] avg_loss per sample: 0.220,   pct succeeded: 0.0%\n",
      "5 -> num sampled [17] avg_loss per sample: 0.156,   pct succeeded: 5.9%\n",
      "6 -> num sampled [14] avg_loss per sample: 0.190,   pct succeeded: 0.0%\n",
      "7 -> num sampled [5] avg_loss per sample: 0.527,   pct succeeded: 20.0%\n",
      "8 -> num sampled [11] avg_loss per sample: 0.240,   pct succeeded: 9.1%\n",
      "epoch 5, avg loss per sample 0.1135, train acc 4.3%, time 50.9 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train(df, model=model_burt_only, optimizer=optim_dict[\"burt_only\"], num_epochs=5):\n",
    "    model = model.to(device)\n",
    "    print('training on', device)\n",
    "    X = df.input_ids\n",
    "    Y = df.labels\n",
    "    batch_count = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('starting epoch#', epoch+1, '...')\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            Y_hat_batch = []\n",
    "            for j in range(len(X[i])): \n",
    "                Y_hat = get_prediction(X[i][j])\n",
    "                Y_hat = Y_hat.flatten().tolist()\n",
    "                Y_hat_batch.append(Y_hat)\n",
    "            \n",
    "            Y_hat_batch = torch.tensor(Y_hat_batch)\n",
    "            labels_batch = Y[i]\n",
    "            labels_batch = torch.tensor(np.array(labels_batch), dtype=torch.int64)\n",
    "\n",
    "            loss = loss_fn(Y_hat_batch, labels_batch)\n",
    "            loss.requires_grad = True\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            num_sample_passed = (Y_hat_batch.argmax(dim=1) == labels_batch).sum().cpu().item()\n",
    "            pct_success = 100*num_sample_passed/labels_batch.shape[0]\n",
    "            \n",
    "            print(\"{} -> num sampled {} avg_loss per sample: {:.3f},   pct succeeded: {:.1f}%\".format(\n",
    "            i, list(labels_batch.size()), loss/labels_batch.shape[0], pct_success))\n",
    "\n",
    "            train_l_sum += loss.cpu().item()\n",
    "            train_acc_sum += (Y_hat_batch.argmax(dim=1) == labels_batch).sum().cpu().item()\n",
    "            n += labels_batch.shape[0]\n",
    "            batch_count += 1 \n",
    "         \n",
    "        print(\"epoch {}, avg loss per sample {:.4f}, train acc {:.1f}%, time {:.1f} sec\\n\".format(\n",
    "            epoch + 1, train_l_sum / n, 100*train_acc_sum / n, time.time() - start))\n",
    "        \n",
    "train(df_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# BioClinicalBert Test Results\n",
    "* All test were conducted with same data of 9 patients with 210 lines in 14 categories\n",
    "## Untrained Raw Predictions without any training\n",
    "* burt_only_pct success 0% , second time pct success 7% \n",
    "* burt_ltsm_pct success 33%, second time pct success 1%\n",
    "\n",
    "## Trained on burt_only ##\n",
    "training on cpu\n",
    "starting epoch# 1 ...\n",
    "0 -> num sampled [52] avg_loss per sample: 0.051,   pct succeeded: 1.9%\n",
    "1 -> num sampled [29] avg_loss per sample: 0.091,   pct succeeded: 0.0%\n",
    "2 -> num sampled [36] avg_loss per sample: 0.074,   pct succeeded: 0.0%\n",
    "3 -> num sampled [34] avg_loss per sample: 0.078,   pct succeeded: 5.9%\n",
    "4 -> num sampled [12] avg_loss per sample: 0.221,   pct succeeded: 0.0%\n",
    "5 -> num sampled [17] avg_loss per sample: 0.156,   pct succeeded: 0.0%\n",
    "6 -> num sampled [14] avg_loss per sample: 0.190,   pct succeeded: 0.0%\n",
    "7 -> num sampled [5] avg_loss per sample: 0.528,   pct succeeded: 0.0%\n",
    "8 -> num sampled [11] avg_loss per sample: 0.241,   pct succeeded: 0.0%\n",
    "epoch 1, avg loss per sample 0.1136, train acc 1.4%, time 50.6 sec\n",
    "\n",
    "starting epoch# 2 ...\n",
    "0 -> num sampled [52] avg_loss per sample: 0.051,   pct succeeded: 3.8%\n",
    "1 -> num sampled [29] avg_loss per sample: 0.091,   pct succeeded: 6.9%\n",
    "2 -> num sampled [36] avg_loss per sample: 0.074,   pct succeeded: 2.8%\n",
    "3 -> num sampled [34] avg_loss per sample: 0.078,   pct succeeded: 5.9%\n",
    "4 -> num sampled [12] avg_loss per sample: 0.221,   pct succeeded: 8.3%\n",
    "5 -> num sampled [17] avg_loss per sample: 0.156,   pct succeeded: 5.9%\n",
    "6 -> num sampled [14] avg_loss per sample: 0.190,   pct succeeded: 0.0%\n",
    "7 -> num sampled [5] avg_loss per sample: 0.526,   pct succeeded: 0.0%\n",
    "8 -> num sampled [11] avg_loss per sample: 0.241,   pct succeeded: 18.2%\n",
    "epoch 2, avg loss per sample 0.1135, train acc 5.2%, time 52.6 sec\n",
    "\n",
    "starting epoch# 3 ...\n",
    "0 -> num sampled [52] avg_loss per sample: 0.051,   pct succeeded: 5.8%\n",
    "1 -> num sampled [29] avg_loss per sample: 0.091,   pct succeeded: 0.0%\n",
    "2 -> num sampled [36] avg_loss per sample: 0.074,   pct succeeded: 0.0%\n",
    "3 -> num sampled [34] avg_loss per sample: 0.078,   pct succeeded: 2.9%\n",
    "4 -> num sampled [12] avg_loss per sample: 0.221,   pct succeeded: 0.0%\n",
    "5 -> num sampled [17] avg_loss per sample: 0.156,   pct succeeded: 5.9%\n",
    "6 -> num sampled [14] avg_loss per sample: 0.189,   pct succeeded: 0.0%\n",
    "7 -> num sampled [5] avg_loss per sample: 0.527,   pct succeeded: 0.0%\n",
    "8 -> num sampled [11] avg_loss per sample: 0.241,   pct succeeded: 9.1%\n",
    "epoch 3, avg loss per sample 0.1135, train acc 2.9%, time 52.5 sec\n",
    "\n",
    "starting epoch# 4 ...\n",
    "0 -> num sampled [52] avg_loss per sample: 0.051,   pct succeeded: 7.7%\n",
    "1 -> num sampled [29] avg_loss per sample: 0.091,   pct succeeded: 3.4%\n",
    "2 -> num sampled [36] avg_loss per sample: 0.074,   pct succeeded: 2.8%\n",
    "3 -> num sampled [34] avg_loss per sample: 0.078,   pct succeeded: 2.9%\n",
    "4 -> num sampled [12] avg_loss per sample: 0.220,   pct succeeded: 8.3%\n",
    "5 -> num sampled [17] avg_loss per sample: 0.156,   pct succeeded: 5.9%\n",
    "6 -> num sampled [14] avg_loss per sample: 0.189,   pct succeeded: 0.0%\n",
    "7 -> num sampled [5] avg_loss per sample: 0.531,   pct succeeded: 0.0%\n",
    "8 -> num sampled [11] avg_loss per sample: 0.240,   pct succeeded: 27.3%\n",
    "epoch 4, avg loss per sample 0.1135, train acc 5.7%, time 50.1 sec\n",
    "\n",
    "starting epoch# 5 ...\n",
    "0 -> num sampled [52] avg_loss per sample: 0.051,   pct succeeded: 7.7%\n",
    "1 -> num sampled [29] avg_loss per sample: 0.091,   pct succeeded: 0.0%\n",
    "2 -> num sampled [36] avg_loss per sample: 0.074,   pct succeeded: 0.0%\n",
    "3 -> num sampled [34] avg_loss per sample: 0.078,   pct succeeded: 5.9%\n",
    "4 -> num sampled [12] avg_loss per sample: 0.220,   pct succeeded: 0.0%\n",
    "5 -> num sampled [17] avg_loss per sample: 0.156,   pct succeeded: 5.9%\n",
    "6 -> num sampled [14] avg_loss per sample: 0.190,   pct succeeded: 0.0%\n",
    "7 -> num sampled [5] avg_loss per sample: 0.527,   pct succeeded: 20.0%\n",
    "8 -> num sampled [11] avg_loss per sample: 0.240,   pct succeeded: 9.1%\n",
    "epoch 5, avg loss per sample 0.1135, train acc 4.3%, time 50.9 sec\n",
    "\n",
    "## Trained on burt_ltsm ##\n",
    "training on cpu\n",
    "starting epoch# 1 ...\n",
    "0 -> num sampled [52] avg_loss per sample: 0.051,   pct succeeded: 3.8%\n",
    "1 -> num sampled [29] avg_loss per sample: 0.092,   pct succeeded: 0.0%\n",
    "2 -> num sampled [36] avg_loss per sample: 0.074,   pct succeeded: 2.8%\n",
    "3 -> num sampled [34] avg_loss per sample: 0.078,   pct succeeded: 0.0%\n",
    "4 -> num sampled [12] avg_loss per sample: 0.221,   pct succeeded: 8.3%\n",
    "5 -> num sampled [17] avg_loss per sample: 0.157,   pct succeeded: 0.0%\n",
    "6 -> num sampled [14] avg_loss per sample: 0.190,   pct succeeded: 0.0%\n",
    "7 -> num sampled [5] avg_loss per sample: 0.527,   pct succeeded: 0.0%\n",
    "8 -> num sampled [11] avg_loss per sample: 0.242,   pct succeeded: 0.0%\n",
    "epoch 1, avg loss per sample 0.1140, train acc 1.9%, time 47.9 sec\n",
    "\n",
    "starting epoch# 2 ...\n",
    "0 -> num sampled [52] avg_loss per sample: 0.051,   pct succeeded: 0.0%\n",
    "1 -> num sampled [29] avg_loss per sample: 0.092,   pct succeeded: 3.4%\n",
    "2 -> num sampled [36] avg_loss per sample: 0.074,   pct succeeded: 2.8%\n",
    "3 -> num sampled [34] avg_loss per sample: 0.078,   pct succeeded: 5.9%\n",
    "4 -> num sampled [12] avg_loss per sample: 0.220,   pct succeeded: 8.3%\n",
    "5 -> num sampled [17] avg_loss per sample: 0.157,   pct succeeded: 0.0%\n",
    "6 -> num sampled [14] avg_loss per sample: 0.190,   pct succeeded: 0.0%\n",
    "7 -> num sampled [5] avg_loss per sample: 0.526,   pct succeeded: 0.0%\n",
    "8 -> num sampled [11] avg_loss per sample: 0.242,   pct succeeded: 0.0%\n",
    "epoch 2, avg loss per sample 0.1139, train acc 2.4%, time 49.1 sec\n",
    "\n",
    "starting epoch# 3 ...\n",
    "0 -> num sampled [52] avg_loss per sample: 0.051,   pct succeeded: 1.9%\n",
    "1 -> num sampled [29] avg_loss per sample: 0.092,   pct succeeded: 0.0%\n",
    "2 -> num sampled [36] avg_loss per sample: 0.074,   pct succeeded: 2.8%\n",
    "3 -> num sampled [34] avg_loss per sample: 0.078,   pct succeeded: 0.0%\n",
    "4 -> num sampled [12] avg_loss per sample: 0.221,   pct succeeded: 0.0%\n",
    "5 -> num sampled [17] avg_loss per sample: 0.157,   pct succeeded: 0.0%\n",
    "6 -> num sampled [14] avg_loss per sample: 0.190,   pct succeeded: 0.0%\n",
    "7 -> num sampled [5] avg_loss per sample: 0.526,   pct succeeded: 0.0%\n",
    "8 -> num sampled [11] avg_loss per sample: 0.242,   pct succeeded: 0.0%\n",
    "epoch 3, avg loss per sample 0.1140, train acc 1.0%, time 51.1 sec\n",
    "\n",
    "starting epoch# 4 ...\n",
    "0 -> num sampled [52] avg_loss per sample: 0.051,   pct succeeded: 0.0%\n",
    "1 -> num sampled [29] avg_loss per sample: 0.092,   pct succeeded: 0.0%\n",
    "2 -> num sampled [36] avg_loss per sample: 0.074,   pct succeeded: 2.8%\n",
    "3 -> num sampled [34] avg_loss per sample: 0.078,   pct succeeded: 0.0%\n",
    "4 -> num sampled [12] avg_loss per sample: 0.221,   pct succeeded: 8.3%\n",
    "5 -> num sampled [17] avg_loss per sample: 0.156,   pct succeeded: 0.0%\n",
    "6 -> num sampled [14] avg_loss per sample: 0.190,   pct succeeded: 0.0%\n",
    "7 -> num sampled [5] avg_loss per sample: 0.526,   pct succeeded: 0.0%\n",
    "8 -> num sampled [11] avg_loss per sample: 0.243,   pct succeeded: 0.0%\n",
    "epoch 4, avg loss per sample 0.1139, train acc 1.0%, time 50.4 sec\n",
    "\n",
    "starting epoch# 5 ...\n",
    "0 -> num sampled [52] avg_loss per sample: 0.051,   pct succeeded: 0.0%\n",
    "1 -> num sampled [29] avg_loss per sample: 0.092,   pct succeeded: 0.0%\n",
    "2 -> num sampled [36] avg_loss per sample: 0.074,   pct succeeded: 2.8%\n",
    "3 -> num sampled [34] avg_loss per sample: 0.078,   pct succeeded: 5.9%\n",
    "4 -> num sampled [12] avg_loss per sample: 0.221,   pct succeeded: 8.3%\n",
    "5 -> num sampled [17] avg_loss per sample: 0.157,   pct succeeded: 0.0%\n",
    "6 -> num sampled [14] avg_loss per sample: 0.190,   pct succeeded: 0.0%\n",
    "7 -> num sampled [5] avg_loss per sample: 0.526,   pct succeeded: 0.0%\n",
    "8 -> num sampled [11] avg_loss per sample: 0.242,   pct succeeded: 9.1%\n",
    "epoch 5, avg loss per sample 0.1139, train acc 2.4%, time 50.7 sec\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugf",
   "language": "python",
   "name": "hugf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
